{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pollen NER+RE Pipeline Notebook (May 2025)\n",
        "# Описание: данный ноутбук реализует полный пайплайн обучения и оценки модели Token Classification (NER+RE) на домене сезонных аллергий."
      ],
      "metadata": {
        "id": "QqpczTPOuOP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PollenNER** — извлечения сущностей для анализа сообщений Пыльца Club\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Заказчик\n",
        "\n",
        "[Пыльца Club](https://pollen.club/) — краудсорсинговый сервис для людей с пыльцевой аллергией. Платформа объединяет тысячи пользователей, информируя их о текущем уровне аллергенов в воздухе и рисках для здоровья. Данные о концентрации пыльцевых частиц и прогнозах доступны через веб-интерфейс и мобильное приложение, что помогает планировать активность и корректировать терапию.\n",
        "\n",
        "\n",
        "\n",
        "## Исполнитель\n",
        "\n",
        "\n",
        "\n",
        "[Мельник Даниил](https://github.com/DanielNRU/)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Описание проекта\n",
        "\n",
        "\n",
        "\n",
        "Цель — создать и обучить NLP-систему, способную извлекать из пользовательских сообщений ключевую информацию о:\n",
        "\n",
        "\n",
        "\n",
        "* **Топонимах**\n",
        "\n",
        "* **Симптомах**\n",
        "\n",
        "* **Медицинских препаратах**\n",
        "\n",
        "* **Аллергенах**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Задачи\n",
        "\n",
        "\n",
        "\n",
        "1. **Подготовка и разметка данных**\n",
        "\n",
        "\n",
        "\n",
        " * Загрузка исторических сообщений пользователей\n",
        "\n",
        " * Очистка и нормализация текста\n",
        "\n",
        " * Аннотирование сущностей в Label Studio\n",
        "\n",
        " * Формирование тренировочной и тестовой выборок\n",
        "\n",
        "\n",
        "\n",
        "2. **Разработка NER-модуля**\n",
        "\n",
        "\n",
        "\n",
        " * Использование предобученной модели **DeepPavlov/rubert-base-cased**\n",
        "\n",
        " * Активное обучение для оптимального расширения аннотаций\n",
        "\n",
        " * Оценка неопределённости для отбора самых информативных примеров\n",
        "\n",
        " * PEFT и LoRA для экономии ресурсов при тонкой настройке\n",
        "\n",
        "\n",
        "\n",
        "3. **Построение RE-модуля**\n",
        "\n",
        "\n",
        "\n",
        " * Модель для классификации отношений между сущностями\n",
        "\n",
        " * Балансировка классов, оптимизация метрик качества\n",
        "\n",
        " * Интеграция в единый пайплайн с NER-модулем\n",
        "\n",
        "\n",
        "\n",
        "4. **Тестирование и внедрение**\n",
        "\n",
        "\n",
        "\n",
        " * Валидация на тестовой выборке\n",
        "\n",
        " * Анализ ошибок и доработка сложных кейсов\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Технологический стек\n",
        "\n",
        "\n",
        "\n",
        "* **Языки и библиотеки**: Python, Pandas, NumPy, scikit-learn\n",
        "\n",
        "* **ML-фреймворки**: PyTorch, Transformers (Hugging Face), PEFT (LoRA)\n",
        "\n",
        "* **Разметка**: Label Studio\n",
        "\n",
        "* **Подходы**: Active Learning, Parameter-Efficient Fine-Tuning\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CXtM17W9kr52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорты библиотек"
      ],
      "metadata": {
        "id": "mWRORxG3uHU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Стандартные библиотеки Python\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "# Научные вычисления и обработка данных\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "import multiprocessing\n",
        "from joblib import parallel_backend\n",
        "\n",
        "# Hugging Face и трансформеры\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForTokenClassification,\n",
        "    Trainer, TrainingArguments, DataCollatorForTokenClassification,\n",
        "    EarlyStoppingCallback, AutoModelForSequenceClassification,\n",
        "    AutoConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Внешние API и сервисы\n",
        "import requests\n",
        "from label_studio_sdk.client import LabelStudio\n",
        "from dotenv import load_dotenv\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "P7jsWDDntzgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Настройка окружения и константы"
      ],
      "metadata": {
        "id": "z9mQVwTSuXw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "RANDOM_STATE = 42\n",
        "LABEL_CONFIG = '''<View>\n",
        "  <Labels name=\"label\" toName=\"text\">\n",
        "    <Label value=\"TOPONYM\" background=\"#ff0d00\"/>\n",
        "    <Label value=\"MEDICINE\" background=\"#022bf7\"/>\n",
        "    <Label value=\"SYMPTOM\" background=\"#ffd500\"/>\n",
        "    <Label value=\"ALLERGEN\" background=\"#00ff55\"/>\n",
        "    <Label value=\"BODY_PART\" background=\"#ff00ff\"/>\n",
        "  </Labels>\n",
        "\n",
        "  <Relations>\n",
        "    <Relation value=\"has_symptom\" />\n",
        "    <Relation value=\"has_medicine\" />\n",
        "  </Relations>\n",
        "\n",
        "  <Text name=\"text\" value=\"$text\" granularity=\"word\"/>\n",
        "</View>'''\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "7fT3StR0u1NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
        "os.environ['LOKY_MAX_CPU_COUNT'] = str(n_jobs)\n",
        "\n",
        "# Подавляем лишние предупреждения, чтобы не засорять вывод.\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', message='No label_names provided for model class')\n",
        "warnings.filterwarnings('ignore', message='Could not find the number of physical cores')\n",
        "\n",
        "# Загружаем переменные окружения из .env файла (токены, URL и т.д.)\n",
        "load_dotenv()\n",
        "LS_TOKEN = os.getenv('LS_LEGACY_TOKEN')  # Токен для Label Studio\n",
        "LS_URL = os.getenv('LS_URL', 'http://localhost:8080')  # URL сервера Label Studio\n",
        "HF_TOKEN = os.getenv('HUGGINGFACE_HUB_TOKEN')  # Токен для HuggingFace Hub\n",
        "assert LS_TOKEN and HF_TOKEN, 'Установите LS_LEGACY_TOKEN и HUGGINGFACE_HUB_TOKEN в .env'\n",
        "\n",
        "# Инициализация клиента для работы с Label Studio через API\n",
        "ls = LabelStudio(base_url=LS_URL, api_key=LS_TOKEN)"
      ],
      "metadata": {
        "id": "UtqVftZ-ugG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Конфигурация меток и токенизатора\n",
        "# Список всех сущностей, которые мы хотим распознавать.\n",
        "LABELS = ['TOPONYM', 'MEDICINE', 'SYMPTOM', 'ALLERGEN', 'BODY_PART']\n",
        "# Словари для преобразования между строковыми и числовыми метками.\n",
        "LABEL2ID = {'O': 0, **{f'B-{l}': i*2+1 for i, l in enumerate(LABELS)}, **{f'I-{l}': i*2+2 for i, l in enumerate(LABELS)}}\n",
        "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "TOKENIZER = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "\n",
        "# Список размеров обучающих выборок для активного обучения.\n",
        "sizes = list(range(50, 1501, 50))\n",
        "\n",
        "# Список допустимых отношений между сущностями для задачи RE.\n",
        "RE_RELATION_LABELS = ['has_symptom', 'has_medicine']\n",
        "\n",
        "# Тестовый пример\n",
        "TEST_EXAMPLES = [\n",
        "    \"В Московской области у меня началась аллергия на пыльцу березы, потекли глаза, нос, принимаю Зиртек и Назонекс. У ребенка в Новокузнецке чешутся глаза, уши и течет нос, врач прописал Кромогексал, Назонекс в нос. В Санкт-Петербурге началось цветение ольхи, сильная реакция, принимаю Эриус, но глаза все равно слезятся.\",\n",
        "]\n",
        "\n",
        "# Определяем функцию для вычисления метрик\n",
        "metric = evaluate.load('seqeval')"
      ],
      "metadata": {
        "id": "NdGm9zH4vjwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Вспомогательные функции"
      ],
      "metadata": {
        "id": "7K6E1Sofu8xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bulk_import_via_http(project_id: int, tasks_list: List[Dict], ls_url: str, ls_token: str, chunk_size: int = 100, max_retries: int = 3):\n",
        "    \"\"\"\n",
        "    Импорт списка задач в Label Studio через HTTP-эндпоинт /import.\n",
        "\n",
        "    Параметры:\n",
        "    - project_id: ID проекта в Label Studio\n",
        "    - tasks_list: список задач для импорта\n",
        "    - ls_url: URL сервера Label Studio\n",
        "    - ls_token: токен доступа\n",
        "    - chunk_size: размер чанка для импорта\n",
        "    - max_retries: максимальное количество попыток при ошибке\n",
        "\n",
        "    Процесс:\n",
        "    1. Разбивает список задач на чанки\n",
        "    2. Отправляет каждый чанк через API\n",
        "    3. При ошибке делает повторные попытки с экспоненциальной задержкой\n",
        "    \"\"\"\n",
        "    url = f\"{ls_url}/api/projects/{project_id}/import\"\n",
        "    headers = {'Authorization': f'Token {ls_token}', 'Content-Type': 'application/json'}\n",
        "    total = len(tasks_list)\n",
        "\n",
        "    # Проходим по списку чанков\n",
        "    for i in range(0, total, chunk_size):\n",
        "        chunk = tasks_list[i:i + chunk_size]\n",
        "        start, end = i + 1, min(i + chunk_size, total)\n",
        "\n",
        "        # Пытаемся отправить с ретрай\n",
        "        for attempt in range(1, max_retries + 1):\n",
        "            resp = requests.post(url, headers=headers, json=chunk)\n",
        "            if resp.ok:\n",
        "                print(f\"Импортировано задач {start}–{end} из {total}\")\n",
        "                break\n",
        "            print(f\"Ошибка импорта {start}–{end}: {resp.status_code}. Попытка {attempt}/{max_retries}\")\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(2 ** attempt)  # Экспоненциальная задержка\n",
        "            else:\n",
        "                print(f\"Не удалось импортировать {start}–{end} после {max_retries} попыток.\")\n",
        "\n",
        "def calculate_uncertainty_scores(model, texts, tokenizer, batch_size=8):\n",
        "    \"\"\"\n",
        "    Рассчитывает оценки неопределенности для каждого текста на основе энтропии предсказаний модели.\n",
        "\n",
        "    Параметры:\n",
        "    - model: модель для предсказаний\n",
        "    - texts: список текстов для оценки\n",
        "    - tokenizer: токенизатор для обработки текстов\n",
        "    - batch_size: размер батча для обработки\n",
        "\n",
        "    Возвращает:\n",
        "    - список оценок неопределенности для каждого текста\n",
        "    \"\"\"\n",
        "    uncertainties = []\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Обработка текстов батчами\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            # Вычисляем вероятности через softmax\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            # Рассчитываем энтропию как меру неопределенности\n",
        "            entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
        "            # Усредняем энтропию по всем токенам в тексте\n",
        "            avg_entropy = entropy.mean(dim=1)\n",
        "            uncertainties.extend(avg_entropy.cpu().numpy())\n",
        "\n",
        "        # Очищаем память GPU\n",
        "        del outputs, logits, probs, entropy, avg_entropy\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return uncertainties\n",
        "\n",
        "def select_uncertain_samples(texts, uncertainties, n_samples=50):\n",
        "    \"\"\"\n",
        "    Выбирает n_samples текстов с наибольшей неопределенностью.\n",
        "    \"\"\"\n",
        "    indices = np.argsort(uncertainties)[-n_samples:]\n",
        "    selected_texts = [texts[i] for i in indices]\n",
        "\n",
        "    # Статистика\n",
        "    print(f\"\\nВыбрано {n_samples} сообщений с оценками неопределенности:\")\n",
        "    print(f\"Минимальная неопределенность: {uncertainties[indices[0]]:.4f}\")\n",
        "    print(f\"Максимальная неопределенность: {uncertainties[indices[-1]]:.4f}\")\n",
        "    print(f\"Средняя неопределенность: {np.mean(uncertainties[indices]):.4f}\")\n",
        "\n",
        "    return selected_texts\n",
        "\n",
        "\n",
        "def retry_on_error(func, max_retries=5, delay=2):\n",
        "    \"\"\"\n",
        "    Декоратор для повторных попыток выполнения функции при ошибках.\n",
        "\n",
        "    Параметры:\n",
        "    - func: функция для выполнения\n",
        "    - max_retries: максимальное количество попыток\n",
        "    - delay: начальная задержка между попытками\n",
        "\n",
        "    Возвращает:\n",
        "    - результат выполнения функции или None при неудаче\n",
        "    \"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    raise e\n",
        "                print(f\"Ошибка при выполнении {func.__name__}: {str(e)}. Попытка {attempt + 1}/{max_retries}\")\n",
        "                time.sleep(delay * (2 ** attempt))  # Экспоненциальная задержка\n",
        "        return None\n",
        "    return wrapper\n",
        "\n",
        "@retry_on_error\n",
        "def create_task(ls_client, project_id, text, text_id):\n",
        "    \"\"\"\n",
        "    Создание задачи в Label Studio с повторными попытками.\n",
        "    \"\"\"\n",
        "    return ls_client.tasks.create(\n",
        "        project=project_id,\n",
        "        data={\n",
        "            'text': text,\n",
        "            'meta': {\n",
        "                'id': str(text_id)\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "\n",
        "@retry_on_error\n",
        "def delete_task(ls_client, task_id):\n",
        "    \"\"\"\n",
        "    Удаление задачи в Label Studio с повторными попытками.\n",
        "    \"\"\"\n",
        "    return ls_client.tasks.delete(task_id)\n",
        "\n",
        "@retry_on_error\n",
        "def get_project_tasks(ls_client, project_id):\n",
        "    \"\"\"\n",
        "    Получение списка задач проекта с повторными попытками.\n",
        "    \"\"\"\n",
        "    return list(ls_client.tasks.list(project=project_id, fields=['data', 'is_labeled']))\n",
        "\n",
        "@retry_on_error\n",
        "def export_project(ls_client, project_id):\n",
        "    \"\"\"\n",
        "    Экспорт проекта с повторными попытками.\n",
        "    \"\"\"\n",
        "    resp = requests.get(\n",
        "        f\"{LS_URL}/api/projects/{project_id}/export?exportType=JSON&download_all_tasks=true\",\n",
        "        headers={'Authorization': f'Token {LS_TOKEN}'}\n",
        "    )\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "@retry_on_error\n",
        "def get_or_create_project(ls_client, title, label_config):\n",
        "    \"\"\"\n",
        "    Получение существующего проекта или создание нового с повторными попытками.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        proj = next((p for p in ls_client.projects.list() if p.title == title), None)\n",
        "        if not proj:\n",
        "            proj = ls_client.projects.create(title=title, label_config=label_config)\n",
        "            print(f\"Создан проект '{title}' (ID={proj.id})\")\n",
        "        return proj\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при работе с проектом '{title}': {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def ensure_project(texts_with_ids, title, export_path, import_prev_json=None):\n",
        "    \"\"\"\n",
        "    Убеждаемся, что в Label Studio есть проект с нужными текстами.\n",
        "\n",
        "    Параметры:\n",
        "    - texts_with_ids: список кортежей (id, text)\n",
        "    - title: название проекта\n",
        "    - export_path: путь для сохранения экспортированных данных\n",
        "    - import_prev_json: путь к файлу с предыдущими аннотациями\n",
        "\n",
        "    Процесс:\n",
        "    1. Проверяет существование проекта\n",
        "    2. Создает новый или использует существующий\n",
        "    3. Импортирует тексты и аннотации\n",
        "    4. Экспортирует размеченные данные\n",
        "\n",
        "    Возвращает:\n",
        "    - список примеров с аннотациями\n",
        "    \"\"\"\n",
        "    # Находим или создаём проект\n",
        "    proj = get_or_create_project(ls, title, LABEL_CONFIG)\n",
        "\n",
        "    # Получаем таски с повторными попытками\n",
        "    tasks = get_project_tasks(ls, proj.id)\n",
        "\n",
        "    # Если число совпадает\n",
        "    if len(tasks) == len(texts_with_ids):\n",
        "        labeled = sum(t.is_labeled for t in tasks)\n",
        "        if labeled == len(texts_with_ids):\n",
        "            print(f\"Все {len(texts_with_ids)} задач размечены в '{title}', экспорт\")\n",
        "            data = export_project(ls, proj.id)\n",
        "            with open(export_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "            examples = []\n",
        "            # Парсим аннотации в нужный формат\n",
        "            for item in data:\n",
        "                tags = []\n",
        "                for ann in item.get('annotations', []):\n",
        "                    for r in ann.get('result', []):\n",
        "                        v = r.get('value')\n",
        "                        if v and isinstance(v, dict) and 'start' in v and 'end' in v and 'labels' in v:\n",
        "                            tags.append({'start': v['start'], 'end': v['end'], 'label': v['labels'][0]})\n",
        "                examples.append({'text': item['data']['text'], 'tags': tags})\n",
        "            print(f\"Экспортировано {len(examples)} примеров в {export_path}\")\n",
        "            return examples\n",
        "        # Если есть неразмеченные — ждём\n",
        "        print(f\"В '{title}' размечено {labeled}/{len(texts_with_ids)}. Заверши разметку.\")\n",
        "        input(\"Нажми Enter после разметки...\")\n",
        "        return ensure_project(texts_with_ids, title, export_path, import_prev_json)\n",
        "\n",
        "    # Иначе — удаляем и создаём заново\n",
        "    if tasks:\n",
        "        for t in tasks:\n",
        "            delete_task(ls, t.id)\n",
        "        print(f\"Очистили {len(tasks)} задач в '{title}'\")\n",
        "\n",
        "    existing = []\n",
        "    # Импорт предыдущих аннотаций, если есть\n",
        "    if import_prev_json and os.path.exists(import_prev_json):\n",
        "        prev = json.load(open(import_prev_json, 'r', encoding='utf-8'))\n",
        "        bulk_import_via_http(proj.id, prev, LS_URL, LS_TOKEN, chunk_size=100)\n",
        "        existing = [i['data']['text'] for i in prev]\n",
        "\n",
        "    # Создаём новые таски для оставшихся текстов\n",
        "    for text_id, text in texts_with_ids:\n",
        "        if text not in existing:\n",
        "            create_task(ls, proj.id, text, text_id)\n",
        "\n",
        "    print(f\"Созданы {len(texts_with_ids) - len(existing)} новых задач в '{title}'\")\n",
        "    input(f\"Разметь их и нажми Enter...\")\n",
        "\n",
        "    # Экспорт и разбираем аннотации\n",
        "    data = export_project(ls, proj.id)\n",
        "    with open(export_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    examples = []\n",
        "    for item in data:\n",
        "        tags = []\n",
        "        for ann in item.get('annotations', []):\n",
        "            for r in ann.get('result', []):\n",
        "                v = r.get('value')\n",
        "                if v and isinstance(v, dict) and 'start' in v and 'end' in v and 'labels' in v:\n",
        "                    tags.append({'start': v['start'], 'end': v['end'], 'label': v['labels'][0]})\n",
        "        examples.append({'text': item['data']['text'], 'tags': tags})\n",
        "\n",
        "    print(f\"Экспортировано {len(examples)} примеров в {export_path}\")\n",
        "    return examples\n",
        "\n",
        "def align_labels(example):\n",
        "    \"\"\"\n",
        "    Привязывает аннотации к токенам текста.\n",
        "\n",
        "    Параметры:\n",
        "    - example: пример с текстом и аннотациями\n",
        "\n",
        "    Возвращает:\n",
        "    - список ID меток для каждого токена\n",
        "    \"\"\"\n",
        "    tok = TOKENIZER(example['text'], return_offsets_mapping=True, truncation=True, max_length=512)\n",
        "    labels = ['O'] * len(tok['offset_mapping'])\n",
        "\n",
        "    # Для каждой аннотации находим соответствующие токены\n",
        "    for tag in example.get('tags', []):\n",
        "        for i, (s, e) in enumerate(tok['offset_mapping']):\n",
        "            if s >= tag['start'] and e <= tag['end']:\n",
        "                # B- для начала сущности, I- для продолжения\n",
        "                labels[i] = ('B-' if s == tag['start'] else 'I-') + tag['label']\n",
        "\n",
        "    return [LABEL2ID[l] for l in labels]\n",
        "\n",
        "def to_hf_dataset(examples):\n",
        "    \"\"\"\n",
        "    Преобразует примеры в формат HuggingFace Dataset.\n",
        "\n",
        "    Параметры:\n",
        "    - examples: список примеров с текстами и аннотациями\n",
        "\n",
        "    Возвращает:\n",
        "    - датасет в формате HuggingFace\n",
        "    - коллатор для батчей\n",
        "    - список текстов\n",
        "    \"\"\"\n",
        "    if not examples: return None, None, []\n",
        "\n",
        "    # Преобразуем в DataFrame\n",
        "    df = pd.DataFrame(examples)\n",
        "    ds = Dataset.from_pandas(df)\n",
        "\n",
        "    # Сохраняем тексты для последующего анализа\n",
        "    texts = df['text'].tolist()\n",
        "\n",
        "    def fn(ex):\n",
        "        # Токенизация и выравнивание меток\n",
        "        tok = TOKENIZER(ex['text'], truncation=True, padding='max_length', max_length=512)\n",
        "        aligned = align_labels(ex)\n",
        "        tok['labels'] = aligned + [-100] * (512 - len(aligned))  # -100 для padding\n",
        "        return tok\n",
        "\n",
        "    ds = ds.map(fn, batched=False, remove_columns=['text','tags'])\n",
        "    return ds, DataCollatorForTokenClassification(TOKENIZER), texts\n",
        "\n",
        "def update_remaining_dataset(remaining_df, json_file):\n",
        "    \"\"\"\n",
        "    Обновляет оставшийся датасет, удаляя из него сообщения, которые были размечены.\n",
        "    Args:\n",
        "        remaining_df: DataFrame с оставшимися сообщениями\n",
        "        json_file: путь к JSON файлу с размеченными данными\n",
        "    Returns:\n",
        "        Обновленный DataFrame с оставшимися сообщениями\n",
        "    \"\"\"\n",
        "    # Удаляем все print, связанные с размером датасета и совпадениями\n",
        "    # Загружаем размеченные данные\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        labeled_data = json.load(f)\n",
        "    # Получаем ID сообщений из JSON файла\n",
        "    labeled_ids = []\n",
        "    for item in labeled_data:\n",
        "        if 'data' in item and 'meta' in item['data'] and 'id' in item['data']['meta']:\n",
        "            labeled_ids.append(str(item['data']['meta']['id']))\n",
        "    if labeled_ids:\n",
        "        # Проверяем, есть ли совпадения ID\n",
        "        matching_ids = remaining_df.index.astype(str).isin(labeled_ids)\n",
        "        # Удаляем строки, где индекс совпадает с ID из JSON\n",
        "        remaining_df = remaining_df[~remaining_df.index.astype(str).isin(labeled_ids)]\n",
        "    return remaining_df\n",
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"\n",
        "    Вычисляет метрики для оценки модели NER.\n",
        "    \"\"\"\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    refs, hyps = [], []\n",
        "    for pr, gt in zip(preds, p.label_ids):\n",
        "        r_seq, h_seq = [], []\n",
        "        for pi, gi in zip(pr, gt):\n",
        "            if gi == -100:\n",
        "                continue\n",
        "            r_seq.append(ID2LABEL[gi])\n",
        "            h_seq.append(ID2LABEL[pi])\n",
        "        refs.append(r_seq)\n",
        "        hyps.append(h_seq)\n",
        "    out = metric.compute(predictions=hyps, references=refs)\n",
        "    return {\n",
        "        'precision': out['overall_precision'],\n",
        "        'recall': out['overall_recall'],\n",
        "        'f1': out['overall_f1']\n",
        "    }\n",
        "\n",
        "def predict_entities(text: str, model, tokenizer, id2label: Dict[int, str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Извлекает именованные сущности из текста с помощью обученной модели.\n",
        "\n",
        "    Args:\n",
        "        text (str): Входной текст для анализа\n",
        "        model: Обученная модель NER\n",
        "        tokenizer: Токенизатор для обработки текста\n",
        "        id2label (Dict[int, str]): Словарь для преобразования ID в метки\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Список словарей с найденными сущностями в формате:\n",
        "            [{'text': 'текст сущности', 'label': 'тип сущности', 'start': начало, 'end': конец}]\n",
        "    \"\"\"\n",
        "    # Определяем устройство для вычислений (GPU/CPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Токенизация текста с сохранением маппинга позиций\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        return_offsets_mapping=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    offset_mapping = inputs.pop('offset_mapping')[0]\n",
        "\n",
        "    # Перенос входных данных на нужное устройство\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Получение предсказаний модели\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits.argmax(-1)[0]\n",
        "\n",
        "    # Перенос предсказаний обратно на CPU для обработки\n",
        "    predictions = predictions.cpu()\n",
        "\n",
        "    # Обработка предсказаний и сбор сущностей\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "\n",
        "    # Проходим по всем токенам и их позициям\n",
        "    for pred, (start, end) in zip(predictions, offset_mapping):\n",
        "        # Пропускаем специальные токены (CLS, SEP, PAD)\n",
        "        if start == 0 and end == 0:\n",
        "            continue\n",
        "\n",
        "        # Получаем метку для текущего токена\n",
        "        label = id2label[pred.item()]\n",
        "\n",
        "        if label.startswith('B-'):\n",
        "            # Если встретили начало новой сущности\n",
        "            if current_entity:\n",
        "                # Сохраняем предыдущую сущность\n",
        "                entities.append(current_entity)\n",
        "            # Создаем новую сущность\n",
        "            current_entity = {\n",
        "                'text': text[start:end],\n",
        "                'label': label[2:],  # Убираем префикс B-\n",
        "                'start': start,\n",
        "                'end': end\n",
        "            }\n",
        "        elif label.startswith('I-') and current_entity and label[2:] == current_entity['label']:\n",
        "            # Продолжаем текущую сущность\n",
        "            current_entity['text'] += text[start:end]\n",
        "            current_entity['end'] = end\n",
        "        else:\n",
        "            # Если встретили токен вне сущности\n",
        "            if current_entity:\n",
        "                # Сохраняем текущую сущность\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "\n",
        "    # Добавляем последнюю сущность, если она есть\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "\n",
        "    return entities\n",
        "\n",
        "def test_model_on_example(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "    Тестирует модель на одном примере и выводит результаты разметки в унифицированном формате.\n",
        "    Для NER модели выводит все определенные сущности.\n",
        "    Для RE модели выводит все сущности, но для SYMPTOM использует только те, что получены из has_symptom.\n",
        "    \"\"\"\n",
        "    print(f\"\\nТестовый пример:\")\n",
        "    print(f\"Текст: {text}\")\n",
        "\n",
        "    # Получаем предсказанные сущности\n",
        "    entities = predict_entities(text, model, tokenizer, ID2LABEL)\n",
        "\n",
        "    # Группируем сущности по типу\n",
        "    ent_by_type = {label: [] for label in LABELS}\n",
        "    for ent in entities:\n",
        "        if ent['label'] in ent_by_type:\n",
        "            ent_by_type[ent['label']].append(ent['text'])\n",
        "\n",
        "    # Выводим результаты NER\n",
        "    print(\"\\n[NER] Результаты анализа:\")\n",
        "    for label in LABELS:\n",
        "        if ent_by_type[label]:\n",
        "            print(f\"{label}: {', '.join(ent_by_type[label])}\")\n",
        "        else:\n",
        "            print(f\"{label}: не найдено\")\n",
        "\n",
        "    # Получаем отношения для извлечения симптомов\n",
        "    relations = []\n",
        "    if hasattr(model, 're_model'):  # Если есть RE модель\n",
        "        entities, relations = infer_ner_re_on_text(text, model, model.re_model, tokenizer, ID2LABEL, model.rel_id2label)\n",
        "\n",
        "        # Собираем симптомы из отношений has_symptom\n",
        "        symptoms_from_relations = []\n",
        "        for rel in relations:\n",
        "            if rel['relation'] == 'has_symptom':\n",
        "                symptoms_from_relations.append(rel['tail']['text'])\n",
        "\n",
        "        # Группируем все сущности по типу\n",
        "        re_ent_by_type = {label: [] for label in LABELS}\n",
        "        for ent in entities:\n",
        "            if ent['label'] in re_ent_by_type:\n",
        "                re_ent_by_type[ent['label']].append(ent['text'])\n",
        "\n",
        "        # Выводим результаты RE\n",
        "        print(\"\\n[RE] Результаты анализа:\")\n",
        "        for label in LABELS:\n",
        "            if label == 'SYMPTOM':\n",
        "                # Для SYMPTOM используем только те, что из has_symptom\n",
        "                if symptoms_from_relations:\n",
        "                    print(f\"{label}: {', '.join(symptoms_from_relations)}\")\n",
        "                else:\n",
        "                    print(f\"{label}: не найдено\")\n",
        "            else:\n",
        "                # Для остальных сущностей используем все найденные\n",
        "                if re_ent_by_type[label]:\n",
        "                    print(f\"{label}: {', '.join(re_ent_by_type[label])}\")\n",
        "                else:\n",
        "                    print(f\"{label}: не найдено\")"
      ],
      "metadata": {
        "id": "J06OxELJvT0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_class_distribution(model, tokenizer, texts, save_path='class_distribution.png'):\n",
        "    \"\"\"\n",
        "    Анализирует и визуализирует распределение классов в наборе текстов.\n",
        "\n",
        "    Параметры:\n",
        "    - model: обученная модель\n",
        "    - tokenizer: токенизатор\n",
        "    - texts: список текстов для анализа\n",
        "    - save_path: путь для сохранения графика\n",
        "    \"\"\"\n",
        "    # Получаем распределение классов\n",
        "    distribution = calculate_class_distribution(texts, tokenizer, model)\n",
        "\n",
        "    if not distribution:\n",
        "        print(\"Не удалось получить распределение классов\")\n",
        "        return\n",
        "\n",
        "    # Создаем DataFrame для визуализации\n",
        "    df = pd.DataFrame({\n",
        "        'Класс': list(distribution.keys()),\n",
        "        'Доля': list(distribution.values())\n",
        "    })\n",
        "\n",
        "    # Сортируем по доле\n",
        "    df = df.sort_values('Доля', ascending=False)\n",
        "\n",
        "    # Создаем график\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    # Основной график распределения\n",
        "    ax = sns.barplot(data=df, x='Класс', y='Доля')\n",
        "\n",
        "    # Настраиваем внешний вид\n",
        "    plt.title('Распределение классов в датасете', pad=20)\n",
        "    plt.xlabel('Класс')\n",
        "    plt.ylabel('Доля')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Добавляем значения над столбцами\n",
        "    for i, v in enumerate(df['Доля']):\n",
        "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # Добавляем информацию о балансе\n",
        "    balance_score = calculate_class_balance_score(distribution)\n",
        "    plt.figtext(0.02, 0.02, f'Оценка баланса классов: {balance_score:.3f}',\n",
        "                fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Выводим статистику\n",
        "    print(\"\\nСтатистика распределения классов:\")\n",
        "    print(f\"Всего классов: {len(distribution)}\")\n",
        "    print(f\"Оценка баланса: {balance_score:.3f}\")\n",
        "    print(\"\\nРаспределение по классам:\")\n",
        "    for label, prob in sorted(distribution.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"- {label}: {prob:.3f}\")\n",
        "\n",
        "def run_cycle(df_path: str, sizes, start_size: Optional[int] = None) -> None:\n",
        "    \"\"\"\n",
        "    Запускает цикл обучения модели NER с активным обучением.\n",
        "\n",
        "    Args:\n",
        "        df_path (str): Путь к исходному датасету\n",
        "        start_size (Optional[int]): Размер выборки, с которой начать обучение\n",
        "    \"\"\"\n",
        "    # Загружаем датасет и создаем копию для оставшихся сообщений\n",
        "    df = pd.read_csv(df_path, sep=';', index_col=0)\n",
        "    print(f\"Загружен исходный датасет размером {len(df)} сообщений\")\n",
        "    print(\"Первые 5 строк исходного датасета:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Если указан start_size, находим его индекс в списке sizes\n",
        "    start_index = 0\n",
        "    if start_size is not None:\n",
        "        try:\n",
        "            start_index = sizes.index(start_size)\n",
        "            print(f\"Начинаем с итерации {start_index + 1} (размер выборки {start_size})\")\n",
        "        except ValueError:\n",
        "            print(f\"Ошибка: размер выборки {start_size} не найден в списке допустимых размеров\")\n",
        "            return\n",
        "\n",
        "    # Проверяем существование тестового проекта\n",
        "    test_project_title = 'PollenNER TEST'\n",
        "    test_project = get_or_create_project(ls, test_project_title, LABEL_CONFIG)\n",
        "    test_tasks = get_project_tasks(ls, test_project.id)\n",
        "\n",
        "    # Если тестовый проект пустой или не существует, создаем его\n",
        "    if not test_tasks:\n",
        "        print(\"\\nСоздание тестового проекта из случайных записей\")\n",
        "        # Выбираем 100 случайных записей с фиксированным seed\n",
        "        test_df = df.sample(n=100, random_state=SEED)\n",
        "        test_texts_with_ids = [(idx, text) for idx, text in zip(test_df.index, test_df['text'])]\n",
        "\n",
        "        # Создаем задачи в Label Studio\n",
        "        for text_id, text in test_texts_with_ids:\n",
        "            create_task(ls, test_project.id, text, text_id)\n",
        "\n",
        "        print(f\"Создано {len(test_texts_with_ids)} тестовых задач\")\n",
        "        print(\"Пожалуйста, разместите тестовые данные в Label Studio\")\n",
        "        input(\"Нажмите Enter после завершения разметки...\")\n",
        "\n",
        "        # Экспортируем размеченные данные\n",
        "        test_data = export_project(ls, test_project.id)\n",
        "        with open('PollenNER_TEST.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Преобразуем данные в формат для обучения\n",
        "        test_ex = []\n",
        "        for item in test_data:\n",
        "            tags = []\n",
        "            for ann in item.get('annotations', []):\n",
        "                for r in ann.get('result', []):\n",
        "                    v = r.get('value')\n",
        "                    if v and isinstance(v, dict) and 'start' in v and 'end' in v and 'labels' in v:\n",
        "                        tags.append({'start': v['start'], 'end': v['end'], 'label': v['labels'][0]})\n",
        "            test_ex.append({'text': item['data']['text'], 'tags': tags})\n",
        "\n",
        "        # Создаем тестовый датасет\n",
        "        test_ds, _, _ = to_hf_dataset(test_ex)\n",
        "\n",
        "        # Обновляем оставшийся датасет\n",
        "        test_ids = [str(item['data']['meta']['id']) for item in test_data]\n",
        "        remaining_df = df[~df.index.astype(str).isin(test_ids)]\n",
        "        print(f\"Размер оставшегося датасета после создания тестового: {len(remaining_df)}\")\n",
        "    else:\n",
        "        print(\"\\nТестовый проект уже существует, загружаем данные\")\n",
        "        # Загружаем существующие тестовые данные\n",
        "        with open('PollenNER_TEST.json', 'r', encoding='utf-8') as f:\n",
        "            test_data = json.load(f)\n",
        "\n",
        "        # Преобразуем данные в формат для обучения\n",
        "        test_ex = []\n",
        "        for item in test_data:\n",
        "            tags = []\n",
        "            for ann in item.get('annotations', []):\n",
        "                for r in ann.get('result', []):\n",
        "                    v = r.get('value')\n",
        "                    if v and isinstance(v, dict) and 'start' in v and 'end' in v and 'labels' in v:\n",
        "                        tags.append({'start': v['start'], 'end': v['end'], 'label': v['labels'][0]})\n",
        "            test_ex.append({'text': item['data']['text'], 'tags': tags})\n",
        "\n",
        "        test_ds, _, _ = to_hf_dataset(test_ex)\n",
        "        test_ids = [str(item['data']['meta']['id']) for item in test_data]\n",
        "        remaining_df = df[~df.index.astype(str).isin(test_ids)]\n",
        "        print(f\"Размер оставшегося датасета: {len(remaining_df)}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # --- Инициализация базовой модели с dropout через конфиг ---\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        'DeepPavlov/rubert-base-cased',\n",
        "        hidden_dropout_prob=0.3,  # Dropout для регуляризации\n",
        "        attention_probs_dropout_prob=0.3,  # Dropout на внимании\n",
        "        id2label=ID2LABEL,\n",
        "        label2id=LABEL2ID\n",
        "    )\n",
        "    base_model = AutoModelForTokenClassification.from_pretrained('DeepPavlov/rubert-base-cased', config=config)\n",
        "\n",
        "    # Если начинаем не с начала, загружаем последнюю обученную модель\n",
        "    if start_index > 0:\n",
        "        prev_size = sizes[start_index - 1]\n",
        "        model_path = f'models/pollen_ner_{prev_size}'\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"Загружаем модель из {model_path}\")\n",
        "            model = get_peft_model(base_model, LoraConfig(task_type='TOKEN_CLS', r=16, lora_alpha=32, lora_dropout=0.1))\n",
        "            model.load_adapter(model_path, adapter_name=\"default\")\n",
        "            # Перемещаем модель на GPU, если доступен\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            model = model.to(device)\n",
        "            print(f\"Модель перемещена на {device}\")\n",
        "        else:\n",
        "            print(f\"Ошибка: модель {model_path} не найдена\")\n",
        "            return\n",
        "    else:\n",
        "        model = get_peft_model(base_model, LoraConfig(task_type='TOKEN_CLS', r=16, lora_alpha=32, lora_dropout=0.1))\n",
        "        # Перемещаем модель на GPU, если доступен\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        print(f\"Модель перемещена на {device}\")\n",
        "\n",
        "    # Продолжаем с указанной итерации\n",
        "    for i, size in enumerate(sizes[start_index:], start_index):\n",
        "        print(f\"\\n[NER] Начинаем итерацию с размером выборки {size}\")\n",
        "\n",
        "        # Проверяем наличие существующего проекта для текущего размера\n",
        "        title = f'PollenNER TRAIN {size}'\n",
        "        export_file = f'PollenNER_TRAIN_{size}.json'\n",
        "\n",
        "        # Проверяем, существует ли проект и все ли задачи размечены\n",
        "        proj = get_or_create_project(ls, title, LABEL_CONFIG)\n",
        "        tasks = get_project_tasks(ls, proj.id)\n",
        "\n",
        "        if tasks and all(t.is_labeled for t in tasks):\n",
        "            print(f\"Найден существующий размеченный проект для размера {size}\")\n",
        "            # Экспортируем размеченные данные\n",
        "            data = export_project(ls, proj.id)\n",
        "            with open(export_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # Преобразуем данные в формат для обучения\n",
        "            train_ex = []\n",
        "            for item in data:\n",
        "                tags = []\n",
        "                for ann in item.get('annotations', []):\n",
        "                    for r in ann.get('result', []):\n",
        "                        v = r.get('value')\n",
        "                        if v and isinstance(v, dict) and 'start' in v and 'end' in v and 'labels' in v:\n",
        "                            tags.append({'start': v['start'], 'end': v['end'], 'label': v['labels'][0]})\n",
        "                train_ex.append({'text': item['data']['text'], 'tags': tags})\n",
        "        else:\n",
        "            # Если это первая итерация и проект пустой, создаем его с нуля\n",
        "            if i == start_index and not tasks:\n",
        "                print(f\"\\nСоздание первого тренировочного проекта с размером {size}\")\n",
        "                # Выбираем случайные записи с фиксированным seed\n",
        "                train_df = remaining_df.sample(n=size, random_state=SEED)\n",
        "                train_texts_with_ids = [(idx, text) for idx, text in zip(train_df.index, train_df['text'])]\n",
        "\n",
        "                # Создаем задачи в Label Studio\n",
        "                for text_id, text in train_texts_with_ids:\n",
        "                    create_task(ls, proj.id, text, text_id)\n",
        "\n",
        "                print(f\"Создано {len(train_texts_with_ids)} тренировочных задач\")\n",
        "                print(\"Пожалуйста, разместите тренировочные данные в Label Studio\")\n",
        "                input(\"Нажмите Enter после завершения разметки...\")\n",
        "\n",
        "                # Экспортируем размеченные данные\n",
        "                data = export_project(ls, proj.id)\n",
        "                with open(export_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "                # Преобразуем данные в формат для обучения\n",
        "                train_ex = []\n",
        "                for item in data:\n",
        "                    tags = []\n",
        "                    for ann in item.get('annotations', []):\n",
        "                        for r in ann.get('result', []):\n",
        "                            v = r.get('value')\n",
        "                            if v and isinstance(v, dict) and 'start' in v and 'end' in v and 'labels' in v:\n",
        "                                tags.append({'start': v['start'], 'end': v['end'], 'label': v['labels'][0]})\n",
        "                    train_ex.append({'text': item['data']['text'], 'tags': tags})\n",
        "            else:\n",
        "                # Загружаем размеченные данные из предыдущей итерации\n",
        "                prev_size = sizes[i-1] if i > 0 else 0\n",
        "                prev_json = f'PollenNER_TRAIN_{prev_size}.json'\n",
        "                if os.path.exists(prev_json):\n",
        "                    print(f\"Загружаем размеченные данные из {prev_json}\")\n",
        "                    with open(prev_json, 'r', encoding='utf-8') as f:\n",
        "                        prev_data = json.load(f)\n",
        "\n",
        "                    # Создаем список задач для импорта\n",
        "                    tasks_list = []\n",
        "\n",
        "                    # Добавляем размеченные данные из предыдущей итерации\n",
        "                    for item in prev_data:\n",
        "                        tasks_list.append({\n",
        "                            'data': {\n",
        "                                'text': item['data']['text'],\n",
        "                                'meta': {\n",
        "                                    'id': item['data']['meta']['id']\n",
        "                                }\n",
        "                            },\n",
        "                            'annotations': item.get('annotations', [])\n",
        "                        })\n",
        "\n",
        "                    # Импортируем размеченные данные\n",
        "                    if tasks_list:\n",
        "                        print(f\"Импортируем {len(tasks_list)} размеченных сообщений из предыдущей итерации\")\n",
        "                        bulk_import_via_http(proj.id, tasks_list, LS_URL, LS_TOKEN)\n",
        "\n",
        "                # Выбираем наименее уверенные примеры из оставшегося датасета\n",
        "                if len(remaining_df) > 0:\n",
        "                    # Получаем оценки неопределенности для всех оставшихся сообщений\n",
        "                    uncertainties = calculate_uncertainty_scores(model, remaining_df['text'].tolist(), TOKENIZER)\n",
        "\n",
        "                    # Выбираем сообщения с наивысшей неопределенностью\n",
        "                    # Количество новых примеров = текущий размер - предыдущий размер\n",
        "                    n_new_samples = size - prev_size\n",
        "                    selected_indices = select_samples_improved(\n",
        "                        model,\n",
        "                        remaining_df['text'].tolist(),\n",
        "                        TOKENIZER,\n",
        "                        n_samples=n_new_samples,\n",
        "                        remaining_texts=remaining_df['text'].tolist(),\n",
        "                        current_iteration=i\n",
        "                    )\n",
        "\n",
        "                    # Получаем выбранные сообщения с их ID\n",
        "                    selected_df = remaining_df.iloc[selected_indices]\n",
        "                    train_texts = [(idx, text) for idx, text in zip(selected_df.index, selected_df['text'])]\n",
        "\n",
        "                    # Добавляем новые сообщения в проект\n",
        "                    print(f\"Добавляем {len(train_texts)} новых сообщений для разметки\")\n",
        "                    for text_id, text in train_texts:\n",
        "                        create_task(ls, proj.id, text, text_id)\n",
        "\n",
        "                    # Ждем разметки новых данных\n",
        "                    print(f\"\\nРазметьте {len(train_texts)} новых сообщений в Label Studio\")\n",
        "                    input(\"Нажмите Enter после завершения разметки...\")\n",
        "\n",
        "                    # Экспортируем все размеченные данные\n",
        "                    data = export_project(ls, proj.id)\n",
        "                    with open(export_file, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "                    # Преобразуем данные в формат для обучения\n",
        "                    train_ex = []\n",
        "                    for item in data:\n",
        "                        tags = []\n",
        "                        for ann in item.get('annotations', []):\n",
        "                            for r in ann.get('result', []):\n",
        "                                v = r.get('value')\n",
        "                                if v and isinstance(v, dict) and 'start' in v and 'end' in v and 'labels' in v:\n",
        "                                    tags.append({'start': v['start'], 'end': v['end'], 'label': v['labels'][0]})\n",
        "                        train_ex.append({'text': item['data']['text'], 'tags': tags})\n",
        "                else:\n",
        "                    print(\"Больше нет доступных сообщений для обучения\")\n",
        "                    break\n",
        "\n",
        "        # Обновляем оставшийся датасет\n",
        "        remaining_df = update_remaining_dataset(remaining_df, export_file)\n",
        "\n",
        "        print(f\"После итерации {i+1} осталось {len(remaining_df)} сообщений\")\n",
        "\n",
        "        train_ds, coll, train_texts = to_hf_dataset(train_ex)\n",
        "\n",
        "        # Настройка аргументов тренировки\n",
        "        args = TrainingArguments(\n",
        "            output_dir=f'runs/train_{size}',\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=10,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='eval_f1',\n",
        "            greater_is_better=True,\n",
        "            eval_strategy='epoch',\n",
        "            save_strategy='epoch',\n",
        "            save_total_limit=2,\n",
        "            push_to_hub=True,\n",
        "            hub_model_id=f'pollen-ner-{size}',\n",
        "            hub_token=HF_TOKEN,\n",
        "            no_cuda=not torch.cuda.is_available(),\n",
        "            weight_decay=0.01  # L2-регуляризация для борьбы с переобучением\n",
        "        )\n",
        "\n",
        "        # Создаем trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=test_ds,\n",
        "            data_collator=coll,\n",
        "            tokenizer=TOKENIZER,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(\n",
        "                early_stopping_patience=2,  # Уменьшено patience для ранней остановки\n",
        "                early_stopping_threshold=0.001\n",
        "            )]\n",
        "        )\n",
        "\n",
        "        print(f\"[NER] Обучение на {size} примерах\")\n",
        "        trainer.train()\n",
        "        ev = trainer.evaluate()\n",
        "        print(f\"[NER] Результаты: F1 = {ev['eval_f1']:.4f}\")\n",
        "\n",
        "        # Получаем предсказания для тестового набора\n",
        "        predictions = trainer.predict(test_ds)\n",
        "        preds = predictions.predictions.argmax(-1)\n",
        "        labels = predictions.label_ids\n",
        "\n",
        "        # Подготавливаем списки для эталонных и предсказанных последовательностей\n",
        "        refs, hyps = [], []\n",
        "        for pr, gt in zip(preds, labels):\n",
        "            r_seq, h_seq = [], []\n",
        "            for pi, gi in zip(pr, gt):\n",
        "                if gi == -100:\n",
        "                    continue\n",
        "                r_seq.append(ID2LABEL[gi])\n",
        "                h_seq.append(ID2LABEL[pi])\n",
        "            refs.append(r_seq)\n",
        "            hyps.append(h_seq)\n",
        "\n",
        "        # Преобразуем последовательности в плоский формат для classification_report\n",
        "        flat_refs = []\n",
        "        flat_hyps = []\n",
        "        for r_seq, h_seq in zip(refs, hyps):\n",
        "            flat_refs.extend(r_seq)\n",
        "            flat_hyps.extend(h_seq)\n",
        "\n",
        "        # Выводим подробный отчет о классификации\n",
        "        print(\"\\n[NER] Подробный отчет о классификации:\")\n",
        "        print(classification_report(flat_refs, flat_hyps, digits=4))\n",
        "\n",
        "        # Сохраняем лучшую модель\n",
        "        model_save_path = f'models/pollen_ner_{size}'\n",
        "        os.makedirs(model_save_path, exist_ok=True)\n",
        "        trainer.save_model(model_save_path)\n",
        "        print(f\"[NER] Модель сохранена в {model_save_path}\")\n",
        "\n",
        "        # --- Сохраняем только лучшую модель NER ---\n",
        "        best_f1 = -1\n",
        "        best_model_path = 'models/pollen_ner_best'\n",
        "\n",
        "        if ev['eval_f1'] > best_f1:\n",
        "            best_f1 = ev['eval_f1']\n",
        "            os.makedirs(best_model_path, exist_ok=True)\n",
        "            trainer.save_model(best_model_path)\n",
        "            TOKENIZER.save_pretrained(best_model_path)\n",
        "            print(f\"[NER] Лучшая модель за все итерации сохранена в {best_model_path}\")\n",
        "\n",
        "        # Тестируем модель на примерах после каждой итерации\n",
        "        print(\"\\n[NER] Тестирование модели после обучения:\")\n",
        "        test_model_on_example(model, TOKENIZER, TEST_EXAMPLES[0])\n",
        "\n",
        "        results.append({'size': size, 'f1': ev['eval_f1']})\n",
        "\n",
        "        # Загружаем лучшую модель для следующей итерации\n",
        "        model = get_peft_model(base_model, LoraConfig(task_type='TOKEN_CLS', r=16, lora_alpha=32, lora_dropout=0.1))\n",
        "        model.load_adapter(model_save_path, adapter_name=\"default\")\n",
        "        # Перемещаем модель на GPU, если доступен\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        print(f\"Модель перемещена на {device}\")\n",
        "\n",
        "        # После каждой итерации выводим статистику распределения классов по train_texts\n",
        "        from collections import Counter\n",
        "        print(f\"\\nСтатистика распределения классов в тренировочном датасете после итерации {i+1}:\")\n",
        "        all_labels = []\n",
        "        for ex in train_texts:\n",
        "            pass\n",
        "        # Посчитаем по меткам:\n",
        "        if 'train_ex' in locals():\n",
        "            for ex in train_ex:\n",
        "                for tag in ex.get('tags', []):\n",
        "                    all_labels.append(tag['label'])\n",
        "            print(Counter(all_labels))\n",
        "        else:\n",
        "            print('Нет данных для подсчёта статистики по меткам.')\n",
        "\n",
        "    # Визуализация результатов\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    ax = plt.gca()\n",
        "    sns.lineplot(data=results_df, x='size', y='f1', marker='o', ax=ax)\n",
        "    ax.set_title('Зависимость F1 от размера выборки', pad=20)\n",
        "    ax.set_xlabel('Размер обучающей выборки')\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    # Подпись только у максимального значения F1\n",
        "    max_idx = results_df['f1'].idxmax()\n",
        "    max_x = results_df.loc[max_idx, 'size']\n",
        "    max_y = results_df.loc[max_idx, 'f1']\n",
        "    ax.annotate(f'{max_y:.3f}', (max_x, max_y), textcoords=\"offset points\",\n",
        "                xytext=(0,10), ha='center', color='red', fontsize=12, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Выводим график на экран\n",
        "    plt.show()\n",
        "\n",
        "    # Сохраняем график на диск\n",
        "    plt.savefig('learning_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Сохраняем финальный оставшийся датасет\n",
        "    remaining_df.to_csv('remaining_dataset_final.csv', sep=';')\n",
        "    print(f\"\\nФинальный оставшийся датасет сохранен в remaining_dataset_final.csv\")\n",
        "    print(f\"Размер финального датасета: {len(remaining_df)} сообщений\")\n",
        "\n",
        "    # После цикла обучения NER:\n",
        "    last_ner_model = model  # Сохраняем последнюю обученную NER-модель\n",
        "\n",
        "    # Загружаем последнюю сохранённую модель из директории\n",
        "    last_size = sizes[-1] if start_size is None else sizes[start_index + len(results) - 1]\n",
        "    model_save_path = f'models/pollen_ner_{last_size}'\n",
        "    base_model = AutoModelForTokenClassification.from_pretrained('DeepPavlov/rubert-base-cased', id2label=ID2LABEL, label2id=LABEL2ID)\n",
        "    last_ner_model = get_peft_model(base_model, LoraConfig(task_type='TOKEN_CLS', r=16, lora_alpha=32, lora_dropout=0.1))\n",
        "    last_ner_model.load_adapter(model_save_path, adapter_name=\"default\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    last_ner_model = last_ner_model.to(device)\n",
        "    return last_ner_model\n",
        "\n",
        "def test_model_on_text(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "    Тестирует модель на новом тексте и выводит результаты разметки.\n",
        "\n",
        "    Параметры:\n",
        "    - model: обученная модель\n",
        "    - tokenizer: токенизатор\n",
        "    - text: текст для тестирования\n",
        "\n",
        "    Выводит:\n",
        "    - разметку текста с выделением найденных сущностей\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits.argmax(-1)[0]\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    labels = [ID2LABEL[p.item()] for p in predictions]\n",
        "\n",
        "    print(\"\\nРезультаты разметки текста:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Текст: {text}\")\n",
        "    print(\"\\nРазметка:\")\n",
        "    current_entity = None\n",
        "    current_text = \"\"\n",
        "\n",
        "    # Обработка токенов и меток\n",
        "    for token, label in zip(tokens, labels):\n",
        "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {current_text.strip()}\")\n",
        "            current_entity = label[2:]\n",
        "            current_text = token.replace(\"##\", \"\")\n",
        "        elif label.startswith(\"I-\"):\n",
        "            current_text += \" \" + token.replace(\"##\", \"\")\n",
        "        else:\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {current_text.strip()}\")\n",
        "                current_entity = None\n",
        "                current_text = \"\"\n",
        "\n",
        "    if current_entity:\n",
        "        print(f\"{current_entity}: {current_text.strip()}\")\n",
        "\n",
        "def calculate_class_diversity(text, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Рассчитывает разнообразие классов в тексте на основе предсказаний модели.\n",
        "\n",
        "    Параметры:\n",
        "    - text: текст для анализа\n",
        "    - tokenizer: токенизатор\n",
        "    - model: модель для предсказаний\n",
        "\n",
        "    Возвращает:\n",
        "    - количество уникальных классов в тексте\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits.argmax(-1)[0]\n",
        "\n",
        "    # Собираем уникальные классы (исключая O)\n",
        "    unique_classes = set()\n",
        "    for pred in predictions:\n",
        "        label = ID2LABEL[pred.item()]\n",
        "        if label != 'O':\n",
        "            unique_classes.add(label.split('-')[1])  # Убираем B-/I- префикс\n",
        "\n",
        "    return len(unique_classes)\n",
        "\n",
        "def calculate_text_diversity(texts, n_clusters=5):\n",
        "    \"\"\"\n",
        "    Рассчитывает разнообразие текстов с помощью кластеризации TF-IDF.\n",
        "\n",
        "    Параметры:\n",
        "    - texts: список текстов\n",
        "    - n_clusters: количество кластеров для кластеризации\n",
        "\n",
        "    Возвращает:\n",
        "    - оценку разнообразия текстов (0-1)\n",
        "    \"\"\"\n",
        "    with parallel_backend('loky', n_jobs=n_jobs):\n",
        "        # Векторизация текстов\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "        # Кластеризация текстов\n",
        "        kmeans = KMeans(\n",
        "            n_clusters=min(n_clusters, len(texts)),\n",
        "            random_state=42\n",
        "        )\n",
        "        clusters = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "        # Анализ распределения текстов по кластерам\n",
        "        cluster_counts = Counter(clusters)\n",
        "\n",
        "        # Нормализация счетчиков для получения оценки разнообразия\n",
        "        total = len(texts)\n",
        "        diversity_scores = [1 - (count/total) for count in cluster_counts.values()]\n",
        "\n",
        "        return np.mean(diversity_scores)\n",
        "\n",
        "def calculate_class_balance(text, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Рассчитывает баланс классов в тексте.\n",
        "\n",
        "    Параметры:\n",
        "    - text: текст для анализа\n",
        "    - tokenizer: токенизатор\n",
        "    - model: модель для предсказаний\n",
        "\n",
        "    Возвращает:\n",
        "    - нормализованную энтропию распределения классов (0-1)\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits.argmax(-1)[0]\n",
        "\n",
        "    # Подсчет количества каждого класса\n",
        "    class_counts = Counter()\n",
        "    for pred in predictions:\n",
        "        label = ID2LABEL[pred.item()]\n",
        "        if label != 'O':\n",
        "            class_counts[label.split('-')[1]] += 1\n",
        "\n",
        "    if not class_counts:\n",
        "        return 0\n",
        "\n",
        "    # Расчет энтропии распределения классов\n",
        "    total = sum(class_counts.values())\n",
        "    probs = [count/total for count in class_counts.values()]\n",
        "    entropy = -sum(p * np.log(p) for p in probs)\n",
        "\n",
        "    # Нормализация энтропии\n",
        "    max_entropy = np.log(len(LABELS))\n",
        "    return entropy / max_entropy if max_entropy > 0 else 0\n",
        "\n",
        "def calculate_class_distribution(texts, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Рассчитывает распределение классов в наборе текстов.\n",
        "\n",
        "    Параметры:\n",
        "    - texts: список текстов\n",
        "    - tokenizer: токенизатор\n",
        "    - model: модель для предсказаний\n",
        "\n",
        "    Возвращает:\n",
        "    - словарь с распределением классов\n",
        "    \"\"\"\n",
        "    class_counts = Counter()\n",
        "    total_tokens = 0\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            predictions = outputs.logits.argmax(-1)[0]\n",
        "\n",
        "        for pred in predictions:\n",
        "            label = ID2LABEL[pred.item()]\n",
        "            if label != 'O':\n",
        "                class_counts[label.split('-')[1]] += 1\n",
        "                total_tokens += 1\n",
        "\n",
        "    return {k: v/total_tokens for k, v in class_counts.items()} if total_tokens > 0 else {}\n",
        "\n",
        "def calculate_class_balance_score(class_distribution):\n",
        "    \"\"\"\n",
        "    Рассчитывает оценку баланса классов.\n",
        "\n",
        "    Параметры:\n",
        "    - class_distribution: распределение классов\n",
        "\n",
        "    Возвращает:\n",
        "    - оценку баланса (0-1)\n",
        "    \"\"\"\n",
        "    if not class_distribution:\n",
        "        return 0\n",
        "\n",
        "    # Расчет энтропии распределения\n",
        "    probs = list(class_distribution.values())\n",
        "    entropy = -sum(p * np.log(p) for p in probs)\n",
        "\n",
        "    # Нормализация энтропии\n",
        "    max_entropy = np.log(len(LABELS))\n",
        "    return entropy / max_entropy if max_entropy > 0 else 0\n",
        "\n",
        "def select_samples_improved(model, texts, tokenizer, n_samples=50, remaining_texts=None, current_iteration=0):\n",
        "    \"\"\"\n",
        "    Улучшенная стратегия отбора сообщений для активного обучения.\n",
        "    Учитывает неопределенность модели, разнообразие текстов и баланс классов.\n",
        "\n",
        "    Параметры:\n",
        "    - model: текущая модель\n",
        "    - texts: список текстов для отбора\n",
        "    - tokenizer: токенизатор\n",
        "    - n_samples: количество сообщений для отбора\n",
        "    - remaining_texts: оставшиеся тексты для учета разнообразия\n",
        "    - current_iteration: текущая итерация обучения\n",
        "\n",
        "    Возвращает:\n",
        "    - индексы отобранных сообщений\n",
        "    \"\"\"\n",
        "    # Веса для метрик\n",
        "    weights = {\n",
        "        'uncertainty': 0.5,      # Неопределенность модели\n",
        "        'text_diversity': 0.2,   # Разнообразие текстов\n",
        "        'class_balance': 0.3     # Баланс классов\n",
        "    }\n",
        "\n",
        "    # Получаем оценки неопределенности\n",
        "    uncertainties = calculate_uncertainty_scores(model, texts, tokenizer)\n",
        "\n",
        "    # Рассчитываем разнообразие текстов\n",
        "    n_clusters = min(5, max(2, len(texts) // 10))\n",
        "    if remaining_texts:\n",
        "        text_diversity = calculate_text_diversity(texts + remaining_texts, n_clusters=n_clusters)\n",
        "    else:\n",
        "        text_diversity = calculate_text_diversity(texts, n_clusters=n_clusters)\n",
        "\n",
        "    # Рассчитываем текущее распределение классов\n",
        "    current_distribution = calculate_class_distribution(texts, tokenizer, model)\n",
        "    class_balance = calculate_class_balance_score(current_distribution)\n",
        "\n",
        "    # Нормализация неопределенности\n",
        "    uncertainties = np.array(uncertainties)\n",
        "    uncertainties = (uncertainties - uncertainties.min()) / (uncertainties.max() - uncertainties.min() + 1e-10)\n",
        "\n",
        "    # Комбинирование метрик с весами\n",
        "    combined_scores = (\n",
        "        weights['uncertainty'] * uncertainties +\n",
        "        weights['text_diversity'] * text_diversity +\n",
        "        weights['class_balance'] * class_balance\n",
        "    )\n",
        "\n",
        "    # Выбор сообщений с наивысшими комбинированными оценками\n",
        "    selected_indices = np.argsort(combined_scores)[-n_samples:]\n",
        "\n",
        "    # Вывод подробной статистики\n",
        "    print(\"\\nСтатистика отобранных сообщений:\")\n",
        "    print(f\"Итерация: {current_iteration}\")\n",
        "    print(f\"Веса метрик:\")\n",
        "    for metric, weight in weights.items():\n",
        "        print(f\"- {metric}: {weight:.3f}\")\n",
        "    print(f\"Средняя неопределенность: {np.mean(uncertainties[selected_indices]):.4f}\")\n",
        "    print(f\"Разнообразие текстов: {text_diversity:.4f}\")\n",
        "    print(f\"Баланс классов: {class_balance:.4f}\")\n",
        "    print(\"\\nРаспределение классов:\")\n",
        "    for label, prob in current_distribution.items():\n",
        "        print(f\"- {label}: {prob:.3f}\")\n",
        "\n",
        "    return selected_indices\n",
        "\n",
        "def analyze_relations_distribution(relations_list):\n",
        "    \"\"\"\n",
        "    Анализирует распределение типов отношений в наборе данных.\n",
        "\n",
        "    Параметры:\n",
        "    - relations_list: список отношений\n",
        "\n",
        "    Возвращает:\n",
        "    - словарь с распределением типов отношений\n",
        "    \"\"\"\n",
        "    relation_counts = Counter()\n",
        "    for relations in relations_list:\n",
        "        for rel in relations:\n",
        "            relation_counts[rel['relation']] += 1\n",
        "\n",
        "    total = sum(relation_counts.values())\n",
        "    return {k: v/total for k, v in relation_counts.items()} if total > 0 else {}\n",
        "\n",
        "def visualize_relations_distribution(relations_list, save_path='relations_distribution.png'):\n",
        "    \"\"\"\n",
        "    Визуализирует распределение типов отношений.\n",
        "\n",
        "    Параметры:\n",
        "    - relations_list: список отношений\n",
        "    - save_path: путь для сохранения графика\n",
        "    \"\"\"\n",
        "    distribution = analyze_relations_distribution(relations_list)\n",
        "\n",
        "    if not distribution:\n",
        "        print(\"Не удалось получить распределение отношений\")\n",
        "        return\n",
        "\n",
        "    # Создаем DataFrame для визуализации\n",
        "    df = pd.DataFrame({\n",
        "        'Тип отношения': list(distribution.keys()),\n",
        "        'Доля': list(distribution.values())\n",
        "    })\n",
        "\n",
        "    # Сортируем по доле\n",
        "    df = df.sort_values('Доля', ascending=False)\n",
        "\n",
        "    # Создаем график\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    # Основной график распределения\n",
        "    ax = sns.barplot(data=df, x='Тип отношения', y='Доля')\n",
        "\n",
        "    # Настраиваем внешний вид\n",
        "    plt.title('Распределение типов отношений', pad=20)\n",
        "    plt.xlabel('Тип отношения')\n",
        "    plt.ylabel('Доля')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Добавляем значения над столбцами\n",
        "    for i, v in enumerate(df['Доля']):\n",
        "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Выводим статистику\n",
        "    print(\"\\nСтатистика распределения отношений:\")\n",
        "    print(f\"Всего типов отношений: {len(distribution)}\")\n",
        "    print(\"\\nРаспределение по типам:\")\n",
        "    for rel_type, prob in sorted(distribution.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"- {rel_type}: {prob:.3f}\")\n",
        "\n",
        "def parse_labelstudio_json(json_path):\n",
        "    \"\"\"\n",
        "    Извлекает сущности и отношения из разметки Label Studio (JSON).\n",
        "    Поддерживает только отношения из RE_RELATION_LABELS.\n",
        "    Пропускает примеры без сущностей или с одной сущностью.\n",
        "\n",
        "    Важно: для RE критично, чтобы id в отношениях (from_id, to_id) совпадали с id сущностей.\n",
        "    Также важно учитывать направленность отношений (from_id -> to_id).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    for item in data:\n",
        "        text = item['data']['text']\n",
        "        ann = item['annotations'][0]['result'] if item['annotations'] and 'result' in item['annotations'][0] else []\n",
        "        entities = []\n",
        "        relations = []\n",
        "        for obj in ann:\n",
        "            if obj.get('type') == 'labels':\n",
        "                ent = {\n",
        "                    'id': obj['id'],\n",
        "                    'start': obj['value']['start'],\n",
        "                    'end': obj['value']['end'],\n",
        "                    'label': obj['value']['labels'][0],\n",
        "                    'text': obj['value']['text']\n",
        "                }\n",
        "                entities.append(ent)\n",
        "        for obj in ann:\n",
        "            if obj.get('type') == 'relation':\n",
        "                if 'labels' in obj:\n",
        "                    rel_label = obj['labels'][0]\n",
        "                elif 'label' in obj:\n",
        "                    rel_label = obj['label']\n",
        "                else:\n",
        "                    rel_label = 'unknown_relation'\n",
        "                if rel_label in RE_RELATION_LABELS:\n",
        "                    rel = {\n",
        "                        'from_id': obj['from_id'],\n",
        "                        'to_id': obj['to_id'],\n",
        "                        'label': rel_label\n",
        "                    }\n",
        "                    relations.append(rel)\n",
        "        # Пропускаем примеры без сущностей или с одной сущностью\n",
        "        if len(entities) < 2:\n",
        "            continue\n",
        "        results.append({'text': text, 'entities': entities, 'relations': relations})\n",
        "    return results\n",
        "\n",
        "def split_sentences(text):\n",
        "    \"\"\"\n",
        "    Разбивает текст на предложения по точкам, восклицательным и вопросительным знакам.\n",
        "    \"\"\"\n",
        "    return [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]\n",
        "\n",
        "def prepare_re_dataset(parsed_data, relation_labels=None, max_no_relation_ratio=3, oversample_medicine=False):\n",
        "    \"\"\"\n",
        "    Формирует датасет для обучения модели извлечения отношений (RE) с балансировкой класса 'no_relation'.\n",
        "    Использует только отношения из RE_RELATION_LABELS.\n",
        "    Пропускает примеры с одной сущностью.\n",
        "    max_no_relation_ratio: максимальное соотношение 'no_relation' к числу позитивных примеров (например, 3:1).\n",
        "    Пары формируются только между сущностями, находящимися в одном предложении.\n",
        "    oversample_medicine: если True, увеличивает число примеров has_medicine до числа has_symptom (дублированием)\n",
        "    \"\"\"\n",
        "    if relation_labels is None:\n",
        "        relation_labels = RE_RELATION_LABELS.copy()\n",
        "    relation_labels = relation_labels + ['no_relation']\n",
        "\n",
        "    re_examples = []\n",
        "    for item in parsed_data:\n",
        "        text = item['text']\n",
        "        entities = item['entities']\n",
        "        relations = item['relations']\n",
        "        if len(entities) < 2:\n",
        "            continue\n",
        "        # Разбиваем текст на предложения\n",
        "        sentences = split_sentences(text)\n",
        "        # Для каждого предложения ищем сущности, которые в него попадают\n",
        "        for sent in sentences:\n",
        "            sent_start = text.find(sent)\n",
        "            sent_end = sent_start + len(sent)\n",
        "            ents_in_sent = [e for e in entities if e['start'] >= sent_start and e['end'] <= sent_end]\n",
        "            # Генерируем пары только внутри предложения\n",
        "            positive, negative = [], []\n",
        "            for i, ent1 in enumerate(ents_in_sent):\n",
        "                for j, ent2 in enumerate(ents_in_sent):\n",
        "                    if i == j:\n",
        "                        continue\n",
        "                    # Фильтруем только осмысленные пары:\n",
        "                    # BODY_PART–SYMPTOM (has_symptom) и BODY_PART–MEDICINE (has_medicine)\n",
        "                    if ent1['label'] == 'BODY_PART' and ent2['label'] == 'SYMPTOM':\n",
        "                        rel_label = None\n",
        "                        for rel in relations:\n",
        "                            if rel['from_id'] == ent1['id'] and rel['to_id'] == ent2['id']:\n",
        "                                rel_label = rel['label']\n",
        "                                break\n",
        "                        rel_label = rel_label if rel_label in relation_labels else 'no_relation'\n",
        "                        ex = {\n",
        "                            'text': text,\n",
        "                            'entity1': ent1,\n",
        "                            'entity2': ent2,\n",
        "                            'relation': rel_label\n",
        "                        }\n",
        "                        if ex['relation'] == 'no_relation':\n",
        "                            negative.append(ex)\n",
        "                        else:\n",
        "                            positive.append(ex)\n",
        "                    elif ent1['label'] == 'BODY_PART' and ent2['label'] == 'MEDICINE':\n",
        "                        rel_label = None\n",
        "                        for rel in relations:\n",
        "                            if rel['from_id'] == ent1['id'] and rel['to_id'] == ent2['id']:\n",
        "                                rel_label = rel['label']\n",
        "                                break\n",
        "                        rel_label = rel_label if rel_label in relation_labels else 'no_relation'\n",
        "                        ex = {\n",
        "                            'text': text,\n",
        "                            'entity1': ent1,\n",
        "                            'entity2': ent2,\n",
        "                            'relation': rel_label\n",
        "                        }\n",
        "                        if ex['relation'] == 'no_relation':\n",
        "                            negative.append(ex)\n",
        "                        else:\n",
        "                            positive.append(ex)\n",
        "            # Балансируем: не больше max_no_relation_ratio * positive\n",
        "            if max_no_relation_ratio is not None and positive:\n",
        "                negative = random.sample(negative, min(len(negative), max_no_relation_ratio * len(positive)))\n",
        "            # --- Oversample has_medicine ---\n",
        "            if oversample_medicine:\n",
        "                medicine_pos = [ex for ex in positive if ex['relation'] == 'has_medicine']\n",
        "                symptom_count = len([ex for ex in positive if ex['relation'] == 'has_symptom'])\n",
        "                if medicine_pos and symptom_count > 0:\n",
        "                    repeats = max(1, symptom_count // len(medicine_pos))\n",
        "                    positive += medicine_pos * (repeats - 1)\n",
        "            re_examples.extend(positive + negative)\n",
        "    return re_examples, relation_labels\n",
        "\n",
        "def insert_entity_markers(text, ent1, ent2):\n",
        "    \"\"\"\n",
        "    Вставляет специальные маркеры вокруг двух сущностей с указанием их типа.\n",
        "    TYPE]...[/TYPE], где TYPE — тип сущности (например, BODY_PART, SYMPTOM).\n",
        "    Это помогает RE-модели лучше различать роли сущностей в паре.\n",
        "    \"\"\"\n",
        "    # Определяем порядок: сначала более ранняя сущность\n",
        "    if ent1['start'] < ent2['start']:\n",
        "        first, second = ent1, ent2\n",
        "    else:\n",
        "        first, second = ent2, ent1\n",
        "    # Формируем маркеры с типом сущности\n",
        "    first_tag = f'[{first[\"label\"]}]'\n",
        "    first_end_tag = f'[/{first[\"label\"]}]'\n",
        "    second_tag = f'[{second[\"label\"]}]'\n",
        "    second_end_tag = f'[/{second[\"label\"]}]'\n",
        "    # Вставляем маркеры с конца, чтобы не сбить индексы\n",
        "    text_marked = (\n",
        "        text[:second['start']] + second_tag +\n",
        "        text[second['start']:second['end']] + second_end_tag +\n",
        "        text[second['end']:] )\n",
        "    text_marked = (\n",
        "        text_marked[:first['start']] + first_tag +\n",
        "        text_marked[first['start']:first['end']] + first_end_tag +\n",
        "        text_marked[first['end']:] )\n",
        "    return text_marked\n",
        "\n",
        "def prepare_hf_re_dataset(re_examples, tokenizer, label2id, max_length=256):\n",
        "    \"\"\"\n",
        "    Преобразует список примеров RE в HuggingFace Dataset.\n",
        "    Пропускает примеры с relation == 'unknown_relation'.\n",
        "    Пропускает пустые примеры.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for ex in re_examples:\n",
        "        if ex['relation'] == 'unknown_relation':\n",
        "            continue\n",
        "        if not ex['entity1']['text'].strip() or not ex['entity2']['text'].strip():\n",
        "            continue\n",
        "        text_marked = insert_entity_markers(ex['text'], ex['entity1'], ex['entity2'])\n",
        "        rows.append({\n",
        "            'text': text_marked,\n",
        "            'label': label2id[ex['relation']]\n",
        "        })\n",
        "    if not rows:\n",
        "        raise ValueError('Нет валидных примеров для RE!')\n",
        "    df = pd.DataFrame(rows)\n",
        "    def tokenize_fn(ex):\n",
        "        return tokenizer(\n",
        "            ex['text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length\n",
        "        )\n",
        "    ds = Dataset.from_pandas(df)\n",
        "    ds = ds.map(tokenize_fn, batched=False)\n",
        "    return ds\n",
        "\n",
        "def train_and_eval_re_model(train_ds, test_ds, num_labels, label2id, id2label, tokenizer, hf_token=None, output_dir='re_model', epochs=5):\n",
        "    \"\"\"\n",
        "    Обучает и оценивает модель для извлечения отношений (RE) на основе BERT.\n",
        "\n",
        "    Параметры:\n",
        "        train_ds: HuggingFace Dataset для обучения\n",
        "        test_ds: HuggingFace Dataset для теста\n",
        "        num_labels: число классов (отношений)\n",
        "        label2id, id2label: словари метка<->id\n",
        "        tokenizer: токенизатор\n",
        "        hf_token: токен для HuggingFace Hub (если нужен пуш)\n",
        "        output_dir: директория для сохранения модели\n",
        "        epochs: число эпох обучения\n",
        "\n",
        "    Возвращает:\n",
        "        trainer, eval_results: Trainer и результаты оценки\n",
        "    \"\"\"\n",
        "    # Загружаем базовую модель (ruBERT)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        'DeepPavlov/rubert-base-cased',\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "    # Аргументы тренировки для RE\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=5,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='eval_f1',\n",
        "        greater_is_better=True,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        save_total_limit=2,\n",
        "        push_to_hub=bool(hf_token),\n",
        "        hub_model_id='pollen-re-model',\n",
        "        hub_token=hf_token,\n",
        "        no_cuda=not torch.cuda.is_available(),\n",
        "        weight_decay=0.01  # L2-регуляризация для RE\n",
        "    )\n",
        "    # Метрика\n",
        "    metric = evaluate.load('f1')\n",
        "    def compute_metrics(p):\n",
        "        preds = np.argmax(p.predictions, axis=1)\n",
        "        return metric.compute(predictions=preds, references=p.label_ids, average='macro')\n",
        "    # Trainer с EarlyStopping для RE\n",
        "    from transformers import EarlyStoppingCallback\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=test_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "    # Обучение\n",
        "    trainer.train()\n",
        "    # Оценка\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"\\nRE-модель: Macro F1 = {eval_results['eval_f1']:.4f}\")\n",
        "    # Сохраняем модель\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    trainer.save_model(output_dir)\n",
        "    print(f\"RE-модель сохранена в {output_dir}\")\n",
        "    return trainer, eval_results\n",
        "\n",
        "def infer_ner_re_on_text(text, ner_model, re_model, tokenizer, id2label_ner, id2label_re):\n",
        "    \"\"\"\n",
        "    Извлекает сущности и отношения из текста с помощью обученных моделей NER и RE.\n",
        "    Возвращает список сущностей и отношений.\n",
        "    Перебирает только допустимые пары (BODY_PART–SYMPTOM и BODY_PART–MEDICINE) и только внутри одного предложения.\n",
        "    \"\"\"\n",
        "    # 1. Извлекаем сущности\n",
        "    entities = predict_entities(text, ner_model, tokenizer, id2label_ner)\n",
        "    # 2. Разбиваем текст на предложения\n",
        "    sentences = split_sentences(text)\n",
        "    relations = []\n",
        "    for sent in sentences:\n",
        "        sent_start = text.find(sent)\n",
        "        sent_end = sent_start + len(sent)\n",
        "        ents_in_sent = [e for e in entities if e['start'] >= sent_start and e['end'] <= sent_end]\n",
        "        # 3. Перебираем только допустимые пары внутри предложения\n",
        "        for ent1 in ents_in_sent:\n",
        "            for ent2 in ents_in_sent:\n",
        "                if ent1 == ent2:\n",
        "                    continue\n",
        "                # Только BODY_PART–SYMPTOM и BODY_PART–MEDICINE\n",
        "                if ent1['label'] == 'BODY_PART' and ent2['label'] == 'SYMPTOM':\n",
        "                    marked_text = insert_entity_markers(text, ent1, ent2)\n",
        "                    inputs = tokenizer(marked_text, return_tensors='pt', truncation=True, max_length=256)\n",
        "                    inputs = {k: v.to(next(re_model.parameters()).device) for k, v in inputs.items()}\n",
        "                    with torch.no_grad():\n",
        "                        logits = re_model(**inputs).logits\n",
        "                        pred = logits.argmax(-1).item()\n",
        "                        rel_label = id2label_re[pred]\n",
        "                    if rel_label != 'no_relation':\n",
        "                        relations.append({'head': ent1, 'tail': ent2, 'relation': rel_label})\n",
        "                elif ent1['label'] == 'BODY_PART' and ent2['label'] == 'MEDICINE':\n",
        "                    marked_text = insert_entity_markers(text, ent1, ent2)\n",
        "                    inputs = tokenizer(marked_text, return_tensors='pt', truncation=True, max_length=256)\n",
        "                    inputs = {k: v.to(next(re_model.parameters()).device) for k, v in inputs.items()}\n",
        "                    with torch.no_grad():\n",
        "                        logits = re_model(**inputs).logits\n",
        "                        pred = logits.argmax(-1).item()\n",
        "                        rel_label = id2label_re[pred]\n",
        "                    if rel_label != 'no_relation':\n",
        "                        relations.append({'head': ent1, 'tail': ent2, 'relation': rel_label})\n",
        "    return entities, relations"
      ],
      "metadata": {
        "id": "NFJexHKPktqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запуск пайплайна"
      ],
      "metadata": {
        "id": "rC07XiJqwxsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    Основной блок выполнения программы.\n",
        "    Последовательно запускаем все этапы обработки данных и обучения моделей.\n",
        "    \"\"\"\n",
        "    # ===============================\n",
        "    # 1. Обучение NER-модели\n",
        "    # ===============================\n",
        "    print(\"\\n=== Начинаем обучение NER-модели ===\")\n",
        "    # Запускаем цикл обучения NER с активным обучением\n",
        "    last_ner_model = run_cycle('dataset_v1.csv', sizes)\n",
        "    print(\"\\nNER-модель обучена успешно!\")\n",
        "\n",
        "    # ===============================\n",
        "    # 2. Обучение RE-модели\n",
        "    # ===============================\n",
        "    print(\"\\n=== Начинаем обучение RE-модели ===\")\n",
        "\n",
        "    # 2.1. Подготовка данных\n",
        "    print('\\n[RE] Этап 1: Подготовка данных')\n",
        "    # Ищем последний существующий TRAIN json для обучения\n",
        "    last_train_json = None\n",
        "    for size in reversed(sizes):\n",
        "        candidate = f'PollenNER_TRAIN_{size}.json'\n",
        "        if os.path.exists(candidate):\n",
        "            last_train_json = candidate\n",
        "            break\n",
        "    if last_train_json is None:\n",
        "        raise FileNotFoundError('Не найден ни один TRAIN json для RE!')\n",
        "    print(f'[RE] Используется тренировочный json: {last_train_json}')\n",
        "\n",
        "    # Парсим разметку из JSON файлов\n",
        "    train_parsed = parse_labelstudio_json(last_train_json)\n",
        "    test_parsed = parse_labelstudio_json('PollenNER_TEST.json')\n",
        "\n",
        "    # 2.2. Подготовка датасетов\n",
        "    print('\\n[RE] Этап 2: Подготовка датасетов')\n",
        "    # Балансируем классы и применяем oversampling для has_medicine\n",
        "    re_train, rel_labels = prepare_re_dataset(\n",
        "        train_parsed,\n",
        "        relation_labels=RE_RELATION_LABELS,\n",
        "        max_no_relation_ratio=1,\n",
        "        oversample_medicine=True\n",
        "    )\n",
        "    re_test, _ = prepare_re_dataset(\n",
        "        test_parsed,\n",
        "        relation_labels=RE_RELATION_LABELS + ['no_relation'],\n",
        "        max_no_relation_ratio=1,\n",
        "        oversample_medicine=False\n",
        "    )\n",
        "\n",
        "    # Выводим статистику распределения классов\n",
        "    print('\\n[RE] Статистика распределения классов:')\n",
        "    print('Train (после сэмплирования):', Counter([ex['relation'] for ex in re_train]))\n",
        "    print('Test:', Counter([ex['relation'] for ex in re_test]))\n",
        "\n",
        "    # Создаем словари для преобразования меток\n",
        "    rel_label2id = {l: i for i, l in enumerate(rel_labels)}\n",
        "    rel_id2label = {i: l for l, i in rel_label2id.items()}\n",
        "\n",
        "    # 2.3. Токенизация\n",
        "    print('\\n[RE] Этап 3: Токенизация')\n",
        "    re_train_ds = prepare_hf_re_dataset(re_train, TOKENIZER, rel_label2id)\n",
        "    re_test_ds = prepare_hf_re_dataset(re_test, TOKENIZER, rel_label2id)\n",
        "\n",
        "    # 2.4. Обучение модели\n",
        "    print('\\n[RE] Этап 4: Обучение модели')\n",
        "    trainer_re, eval_results_re = train_and_eval_re_model(\n",
        "        train_ds=re_train_ds,\n",
        "        test_ds=re_test_ds,\n",
        "        num_labels=len(rel_labels),\n",
        "        label2id=rel_label2id,\n",
        "        id2label=rel_id2label,\n",
        "        tokenizer=TOKENIZER,\n",
        "        hf_token=HF_TOKEN,\n",
        "        output_dir='models/pollen_re_model',\n",
        "        epochs=5\n",
        "    )\n",
        "    print(f\"\\n[RE] Результаты обучения:\")\n",
        "    print(f\"Macro F1 на тесте: {eval_results_re['eval_f1']:.4f}\")\n",
        "\n",
        "    # 2.5. Детальный анализ результатов\n",
        "    print('\\n[RE] Этап 5: Детальный анализ результатов')\n",
        "    # Получаем предсказания для тестового набора\n",
        "    re_test_preds = trainer_re.predict(re_test_ds)\n",
        "    y_true = re_test_preds.label_ids\n",
        "    y_pred = re_test_preds.predictions.argmax(-1)\n",
        "\n",
        "    # Преобразуем индексы в метки\n",
        "    y_true_labels = [rel_id2label[i] for i in y_true]\n",
        "    y_pred_labels = [rel_id2label[i] for i in y_pred]\n",
        "\n",
        "    # Выводим подробный отчет по целевым классам\n",
        "    target_labels = [l for l in rel_labels if l != 'no_relation']\n",
        "    print(\"\\n[RE] Подробный отчет по целевым классам:\")\n",
        "    print(classification_report(y_true_labels, y_pred_labels, labels=target_labels, digits=4, zero_division=0))\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 3. Финальное тестирование\n",
        "    # ===============================\n",
        "    print('\\n=== Финальное тестирование на тестовых примерах ===')\n",
        "    for i, text in enumerate(TEST_EXAMPLES, 1):\n",
        "        print(f'\\nТестовый пример {i}:')\n",
        "        print(f'Текст: {text}')\n",
        "\n",
        "        # Извлекаем сущности и отношения\n",
        "        entities, relations = infer_ner_re_on_text(\n",
        "            text, last_ner_model, trainer_re.model, TOKENIZER, ID2LABEL, rel_id2label\n",
        "        )\n",
        "\n",
        "        # Собираем симптомы из отношений has_symptom\n",
        "        symptoms_from_relations = []\n",
        "        for rel in relations:\n",
        "            if rel['relation'] == 'has_symptom':\n",
        "                symptoms_from_relations.append(rel['tail']['text'])\n",
        "\n",
        "        # Группируем сущности по типу (исключая BODY_PART)\n",
        "        ent_by_type = {label: [] for label in LABELS if label != 'BODY_PART'}\n",
        "        for ent in entities:\n",
        "            if ent['label'] in ent_by_type:\n",
        "                if ent['label'] == 'SYMPTOM':\n",
        "                    # Для SYMPTOM используем только те, что из has_symptom\n",
        "                    if ent['text'] in symptoms_from_relations:\n",
        "                        ent_by_type[ent['label']].append(ent['text'])\n",
        "                else:\n",
        "                    # Для остальных сущностей используем все найденные\n",
        "                    ent_by_type[ent['label']].append(ent['text'])\n",
        "\n",
        "        # Выводим результаты\n",
        "        print('\\nРезультаты анализа:')\n",
        "        for label in ent_by_type.keys():\n",
        "            if ent_by_type[label]:\n",
        "                ents_str = ', '.join(ent_by_type[label])\n",
        "                print(f'{label}: {ents_str}')\n",
        "            else:\n",
        "                print(f'{label}: не найдено')"
      ],
      "metadata": {
        "id": "P9TDiMZIwpbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Вывод\n",
        "\n",
        "Проведённые эксперименты показали, что внедрение отдельного модуля для оценки отношений (RE) существенно повысило точность выделения и классификации симптомов, устранив ложные срабатывания на упоминания частей тела. Уже с 500 размеченными сообщениями наблюдался заметный рост F1‑метрики, а к 1400 примерам модель достигла стабильного уровня в 0.869, демонстрируя зрелость и надёжность алгоритма.\n",
        "\n",
        "Интеграция NER и RE модулей в единую NLP‑конвейерную архитектуру позволяет в реальном времени автоматически обрабатывать пользовательские сообщения Пыльца Club, извлекая топонимы, симптомы, препараты и аллергены и устанавливая между ними семантические связи."
      ],
      "metadata": {
        "id": "u6YRHSqF4BYl"
      }
    }
  ]
}